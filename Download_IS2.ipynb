{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c39ea14-78cc-4f93-ab1e-dabe1f802ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import getopt\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import netrc\n",
    "import os.path\n",
    "import ssl\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import time\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    from urllib.parse import urlparse\n",
    "    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n",
    "    from urllib.error import HTTPError, URLError\n",
    "except ImportError:\n",
    "    from urlparse import urlparse\n",
    "    from urllib2 import urlopen, Request, HTTPError, URLError, build_opener, HTTPCookieProcessor\n",
    "\n",
    "short_name = 'ATL03'\n",
    "version = '006'\n",
    "time_start = '2019-03-13T00:00:00Z'\n",
    "time_end = '2019-03-14T00:00:00Z'\n",
    "bounding_box = '160,-80,-150,-60'\n",
    "polygon = ''\n",
    "filename_filter = ''\n",
    "url_list = []\n",
    "\n",
    "CMR_URL = 'https://cmr.earthdata.nasa.gov'\n",
    "URS_URL = 'https://urs.earthdata.nasa.gov'\n",
    "CMR_PAGE_SIZE = 2000\n",
    "CMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n",
    "                '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n",
    "                '&scroll=true&page_size={1}'.format(CMR_URL, CMR_PAGE_SIZE))\n",
    "\n",
    "\n",
    "def get_username():\n",
    "    username = 'younghyunkoo'\n",
    "\n",
    "#     # For Python 2/3 compatibility:\n",
    "#     try:\n",
    "#         do_input = raw_input  # noqa\n",
    "#     except NameError:\n",
    "#         do_input = input\n",
    "\n",
    "#     username = do_input('Earthdata username (or press Return to use a bearer token): ')\n",
    "    return username\n",
    "\n",
    "\n",
    "def get_password():\n",
    "    password = 'YHch9245'\n",
    "    # while not password:\n",
    "    #     password = getpass('password: ')\n",
    "    return password\n",
    "\n",
    "\n",
    "def get_token():\n",
    "    token = ''\n",
    "    while not token:\n",
    "        token = getpass('bearer token: ')\n",
    "    return token\n",
    "\n",
    "\n",
    "def get_login_credentials():\n",
    "    \"\"\"Get user credentials from .netrc or prompt for input.\"\"\"\n",
    "    credentials = None\n",
    "    token = None\n",
    "\n",
    "    try:\n",
    "        info = netrc.netrc()\n",
    "        username, account, password = info.authenticators(urlparse(URS_URL).hostname)\n",
    "        if username == 'token':\n",
    "            token = password\n",
    "        else:\n",
    "            credentials = '{0}:{1}'.format(username, password)\n",
    "            credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n",
    "    except Exception:\n",
    "        username = None\n",
    "        password = None\n",
    "\n",
    "    if not username:\n",
    "        username = get_username()\n",
    "        if len(username):\n",
    "            password = get_password()\n",
    "            credentials = '{0}:{1}'.format(username, password)\n",
    "            credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n",
    "        else:\n",
    "            token = get_token()\n",
    "\n",
    "    return credentials, token\n",
    "\n",
    "\n",
    "def build_version_query_params(version):\n",
    "    desired_pad_length = 3\n",
    "    if len(version) > desired_pad_length:\n",
    "        print('Version string too long: \"{0}\"'.format(version))\n",
    "        quit()\n",
    "\n",
    "    version = str(int(version))  # Strip off any leading zeros\n",
    "    query_params = ''\n",
    "\n",
    "    while len(version) <= desired_pad_length:\n",
    "        padded_version = version.zfill(desired_pad_length)\n",
    "        query_params += '&version={0}'.format(padded_version)\n",
    "        desired_pad_length -= 1\n",
    "    return query_params\n",
    "\n",
    "\n",
    "def filter_add_wildcards(filter):\n",
    "    if not filter.startswith('*'):\n",
    "        filter = '*' + filter\n",
    "    if not filter.endswith('*'):\n",
    "        filter = filter + '*'\n",
    "    return filter\n",
    "\n",
    "\n",
    "def build_filename_filter(filename_filter):\n",
    "    filters = filename_filter.split(',')\n",
    "    result = '&options[producer_granule_id][pattern]=true'\n",
    "    for filter in filters:\n",
    "        result += '&producer_granule_id[]=' + filter_add_wildcards(filter)\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_cmr_query_url(short_name, version, time_start, time_end,\n",
    "                        bounding_box=None, polygon=None,\n",
    "                        filename_filter=None):\n",
    "    params = '&short_name={0}'.format(short_name)\n",
    "    params += build_version_query_params(version)\n",
    "    params += '&temporal[]={0},{1}'.format(time_start, time_end)\n",
    "    if polygon:\n",
    "        params += '&polygon={0}'.format(polygon)\n",
    "    elif bounding_box:\n",
    "        params += '&bounding_box={0}'.format(bounding_box)\n",
    "    if filename_filter:\n",
    "        params += build_filename_filter(filename_filter)\n",
    "    return CMR_FILE_URL + params\n",
    "\n",
    "\n",
    "def get_speed(time_elapsed, chunk_size):\n",
    "    if time_elapsed <= 0:\n",
    "        return ''\n",
    "    speed = chunk_size / time_elapsed\n",
    "    if speed <= 0:\n",
    "        speed = 1\n",
    "    size_name = ('', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n",
    "    i = int(math.floor(math.log(speed, 1000)))\n",
    "    p = math.pow(1000, i)\n",
    "    return '{0:.1f}{1}B/s'.format(speed / p, size_name[i])\n",
    "\n",
    "\n",
    "def output_progress(count, total, status='', bar_len=60):\n",
    "    if total <= 0:\n",
    "        return\n",
    "    fraction = min(max(count / float(total), 0), 1)\n",
    "    filled_len = int(round(bar_len * fraction))\n",
    "    percents = int(round(100.0 * fraction))\n",
    "    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n",
    "    fmt = '  [{0}] {1:3d}%  {2}   '.format(bar, percents, status)\n",
    "    print('\\b' * (len(fmt) + 4), end='')  # clears the line\n",
    "    sys.stdout.write(fmt)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def cmr_read_in_chunks(file_object, chunk_size=1024 * 1024):\n",
    "    \"\"\"Read a file in chunks using a generator. Default chunk size: 1Mb.\"\"\"\n",
    "    while True:\n",
    "        data = file_object.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n",
    "\n",
    "\n",
    "def get_login_response(url, credentials, token):\n",
    "    opener = build_opener(HTTPCookieProcessor())\n",
    "\n",
    "    req = Request(url)\n",
    "    if token:\n",
    "        req.add_header('Authorization', 'Bearer {0}'.format(token))\n",
    "    elif credentials:\n",
    "        try:\n",
    "            response = opener.open(req)\n",
    "            # We have a redirect URL - try again with authorization.\n",
    "            url = response.url\n",
    "        except HTTPError:\n",
    "            # No redirect - just try again with authorization.\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print('Error{0}: {1}'.format(type(e), str(e)))\n",
    "            sys.exit(1)\n",
    "\n",
    "        req = Request(url)\n",
    "        req.add_header('Authorization', 'Basic {0}'.format(credentials))\n",
    "\n",
    "    try:\n",
    "        response = opener.open(req)\n",
    "    except HTTPError as e:\n",
    "        err = 'HTTP error {0}, {1}'.format(e.code, e.reason)\n",
    "        if 'Unauthorized' in e.reason:\n",
    "            if token:\n",
    "                err += ': Check your bearer token'\n",
    "            else:\n",
    "                err += ': Check your username and password'\n",
    "        print(err)\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print('Error{0}: {1}'.format(type(e), str(e)))\n",
    "        sys.exit(1)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def cmr_download(urls, outfolder=\"\", force=False, quiet=False):\n",
    "    \"\"\"Download files from list of urls.\"\"\"\n",
    "    if not urls:\n",
    "        return\n",
    "\n",
    "    url_count = len(urls)\n",
    "    if not quiet:\n",
    "        print('Downloading {0} files...'.format(url_count))\n",
    "    credentials = None\n",
    "    token = None\n",
    "\n",
    "    for index, url in enumerate(urls, start=1):\n",
    "        if not credentials and not token:\n",
    "            p = urlparse(url)\n",
    "            if p.scheme == 'https':\n",
    "                credentials, token = get_login_credentials()\n",
    "\n",
    "        filename = url.split('/')[-1]\n",
    "        if not quiet:\n",
    "            print('{0}/{1}: {2}'.format(str(index).zfill(len(str(url_count))),\n",
    "                                        url_count, filename))\n",
    "\n",
    "        try:\n",
    "            response = get_login_response(url, credentials, token)\n",
    "            length = int(response.headers['content-length'])\n",
    "            try:\n",
    "                if not force and length == os.path.getsize(filename):\n",
    "                    if not quiet:\n",
    "                        print('  File exists, skipping')\n",
    "                    continue\n",
    "            except OSError:\n",
    "                pass\n",
    "            count = 0\n",
    "            chunk_size = min(max(length, 1), 1024 * 1024)\n",
    "            max_chunks = int(math.ceil(length / chunk_size))\n",
    "            time_initial = time.time()\n",
    "            with open(outfolder + filename, 'wb') as out_file:\n",
    "                for data in cmr_read_in_chunks(response, chunk_size=chunk_size):\n",
    "                    out_file.write(data)\n",
    "                    if not quiet:\n",
    "                        count = count + 1\n",
    "                        time_elapsed = time.time() - time_initial\n",
    "                        download_speed = get_speed(time_elapsed, count * chunk_size)\n",
    "                        output_progress(count, max_chunks, status=download_speed)\n",
    "            if not quiet:\n",
    "                print()\n",
    "        except HTTPError as e:\n",
    "            print('HTTP error {0}, {1}'.format(e.code, e.reason))\n",
    "        except URLError as e:\n",
    "            print('URL error: {0}'.format(e.reason))\n",
    "        except IOError:\n",
    "            raise\n",
    "\n",
    "\n",
    "def cmr_filter_urls(search_results):\n",
    "    \"\"\"Select only the desired data files from CMR response.\"\"\"\n",
    "    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n",
    "        return []\n",
    "\n",
    "    entries = [e['links']\n",
    "               for e in search_results['feed']['entry']\n",
    "               if 'links' in e]\n",
    "    # Flatten \"entries\" to a simple list of links\n",
    "    links = list(itertools.chain(*entries))\n",
    "\n",
    "    urls = []\n",
    "    unique_filenames = set()\n",
    "    for link in links:\n",
    "        if 'href' not in link:\n",
    "            # Exclude links with nothing to download\n",
    "            continue\n",
    "        if 'inherited' in link and link['inherited'] is True:\n",
    "            # Why are we excluding these links?\n",
    "            continue\n",
    "        if 'rel' in link and 'data#' not in link['rel']:\n",
    "            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n",
    "            continue\n",
    "\n",
    "        if 'title' in link and 'opendap' in link['title'].lower():\n",
    "            # Exclude OPeNDAP links--they are responsible for many duplicates\n",
    "            # This is a hack; when the metadata is updated to properly identify\n",
    "            # non-datapool links, we should be able to do this in a non-hack way\n",
    "            continue\n",
    "\n",
    "        filename = link['href'].split('/')[-1]\n",
    "        if filename in unique_filenames:\n",
    "            # Exclude links with duplicate filenames (they would overwrite)\n",
    "            continue\n",
    "        unique_filenames.add(filename)\n",
    "\n",
    "        urls.append(link['href'])\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "def cmr_search(short_name, version, time_start, time_end,\n",
    "               bounding_box='', polygon='', filename_filter='', quiet=False):\n",
    "    \"\"\"Perform a scrolling CMR query for files matching input criteria.\"\"\"\n",
    "    cmr_query_url = build_cmr_query_url(short_name=short_name, version=version,\n",
    "                                        time_start=time_start, time_end=time_end,\n",
    "                                        bounding_box=bounding_box,\n",
    "                                        polygon=polygon, filename_filter=filename_filter)\n",
    "    if not quiet:\n",
    "        print('Querying for data:\\n\\t{0}\\n'.format(cmr_query_url))\n",
    "\n",
    "    cmr_scroll_id = None\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "    urls = []\n",
    "    hits = 0\n",
    "    while True:\n",
    "        req = Request(cmr_query_url)\n",
    "        if cmr_scroll_id:\n",
    "            req.add_header('cmr-scroll-id', cmr_scroll_id)\n",
    "        try:\n",
    "            response = urlopen(req, context=ctx)\n",
    "        except Exception as e:\n",
    "            print('Error: ' + str(e))\n",
    "            sys.exit(1)\n",
    "        if not cmr_scroll_id:\n",
    "            # Python 2 and 3 have different case for the http headers\n",
    "            headers = {k.lower(): v for k, v in dict(response.info()).items()}\n",
    "            cmr_scroll_id = headers['cmr-scroll-id']\n",
    "            hits = int(headers['cmr-hits'])\n",
    "            if not quiet:\n",
    "                if hits > 0:\n",
    "                    print('Found {0} matches.'.format(hits))\n",
    "                else:\n",
    "                    print('Found no matches.')\n",
    "        search_page = response.read()\n",
    "        search_page = json.loads(search_page.decode('utf-8'))\n",
    "        url_scroll_results = cmr_filter_urls(search_page)\n",
    "        if not url_scroll_results:\n",
    "            break\n",
    "        if not quiet and hits > CMR_PAGE_SIZE:\n",
    "            print('.', end='')\n",
    "            sys.stdout.flush()\n",
    "        urls += url_scroll_results\n",
    "\n",
    "    if not quiet and hits > CMR_PAGE_SIZE:\n",
    "        print()\n",
    "    return urls\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    global short_name, version, time_start, time_end, bounding_box, \\\n",
    "        polygon, filename_filter, url_list\n",
    "\n",
    "    if argv is None:\n",
    "        argv = sys.argv[1:]\n",
    "\n",
    "    force = False\n",
    "    quiet = False\n",
    "    usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n",
    "\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n",
    "        for opt, _arg in opts:\n",
    "            if opt in ('-f', '--force'):\n",
    "                force = True\n",
    "            elif opt in ('-q', '--quiet'):\n",
    "                quiet = True\n",
    "            elif opt in ('-h', '--help'):\n",
    "                print(usage)\n",
    "                sys.exit(0)\n",
    "    except getopt.GetoptError as e:\n",
    "        print(e.args[0])\n",
    "        print(usage)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Supply some default search parameters, just for testing purposes.\n",
    "    # These are only used if the parameters aren't filled in up above.\n",
    "    if 'short_name' in short_name:\n",
    "        short_name = 'ATL06'\n",
    "        version = '003'\n",
    "        time_start = '2018-10-14T00:00:00Z'\n",
    "        time_end = '2021-01-08T21:48:13Z'\n",
    "        bounding_box = ''\n",
    "        polygon = ''\n",
    "        filename_filter = '*ATL06_2020111121*'\n",
    "        url_list = []\n",
    "\n",
    "    try:\n",
    "        if not url_list:\n",
    "            url_list = cmr_search(short_name, version, time_start, time_end,\n",
    "                                  bounding_box=bounding_box, polygon=polygon,\n",
    "                                  filename_filter=filename_filter, quiet=quiet)\n",
    "\n",
    "        # cmr_download(url_list, force=force, quiet=quiet)\n",
    "        \n",
    "        for url in tqdm(url_list[:]):\n",
    "            if url[-3:] == \".h5\":\n",
    "                try:\n",
    "                    outfolder = \"F:\\\\2022_Ross\\\\ATL10_h5\" # The folder you want to save the ATL10 files (please add \"\\\\\" at the end!!)\n",
    "                    cmr_download([url], outfolder=outfolder, force=force, quiet=True)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        quit()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d904a74e-7ed6-4855-943e-163238356409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for data:\n",
      "\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=ATL03&version=006&version=06&version=6&temporal[]=2019-03-13T00:00:00Z,2019-03-14T00:00:00Z&bounding_box=160,-80,-150,-60\n",
      "\n",
      "Found 10 matches.\n"
     ]
    }
   ],
   "source": [
    "global short_name, version, time_start, time_end, bounding_box, polygon, filename_filter, url_list\n",
    "argv = None\n",
    "if argv is None:\n",
    "    argv = sys.argv[1:]\n",
    "\n",
    "force = False\n",
    "quiet = False\n",
    "usage = 'usage: nsidc-download_***.py [--help, -h] [--force, -f] [--quiet, -q]'\n",
    "\n",
    "try:\n",
    "    opts, args = getopt.getopt(argv, 'hfq', ['help', 'force', 'quiet'])\n",
    "    for opt, _arg in opts:\n",
    "        if opt in ('-f', '--force'):\n",
    "            force = True\n",
    "        elif opt in ('-q', '--quiet'):\n",
    "            quiet = True\n",
    "        elif opt in ('-h', '--help'):\n",
    "            print(usage)\n",
    "            sys.exit(0)\n",
    "except getopt.GetoptError as e:\n",
    "    print(e.args[0])\n",
    "    print(usage)\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "try:\n",
    "    if not url_list:\n",
    "        url_list = cmr_search(short_name, version, time_start, time_end,\n",
    "                              bounding_box=bounding_box, polygon=polygon,\n",
    "                              filename_filter=filename_filter, quiet=quiet)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "794809f0-beeb-46af-b1eb-8cfaa85c0230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313060439_11420210_006_02.h5',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313060439_11420210_006_02.iso.xml',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313061220_11420211_006_02.h5',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313061220_11420211_006_02.iso.xml',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313073856_11430210_006_02.h5',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313073856_11430210_006_02.iso.xml',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313074637_11430211_006_02.h5',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313074637_11430211_006_02.iso.xml',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313091314_11440210_006_02.h5',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313091314_11440210_006_02.iso.xml',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313185222_11500212_006_02.h5',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313185222_11500212_006_02.iso.xml',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313202056_11510211_006_02.h5',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313202056_11510211_006_02.iso.xml',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313202639_11510212_006_02.h5',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313202639_11510212_006_02.iso.xml',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313215514_11520211_006_02.h5',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313215514_11520211_006_02.iso.xml',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313220057_11520212_006_02.h5',\n",
       " 'https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.006/2019.03.13/ATL03_20190313220057_11520212_006_02.iso.xml']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce5bc2d1-59dc-41a4-aa38-de2bcd96b00d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [01:30<00:00,  2.84s/it]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if not url_list:\n",
    "        url_list = cmr_search(short_name, version, time_start, time_end,\n",
    "                              bounding_box=bounding_box, polygon=polygon,\n",
    "                              filename_filter=filename_filter, quiet=quiet)\n",
    "\n",
    "    # cmr_download(url_list, force=force, quiet=quiet)\n",
    "\n",
    "    for url in tqdm(url_list[:]):\n",
    "        if url[-3:] == \".h5\":\n",
    "            try: \n",
    "                outfolder = \"D:\\\\Ross\\\\ATL10_h5\\\\\" # The folder you want to save the ATL10 files (please add \"\\\\\" at the end!!)\n",
    "                cmr_download([url], outfolder=outfolder, force=force, quiet=True)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
