{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca70beb9-b04c-4b94-8512-1a1c85d1a1b8",
   "metadata": {},
   "source": [
    "## Process sea ice topographical features in the Weddell Sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0443d929-198c-46fd-840e-e27e524245f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, glob\n",
    "import csv\n",
    "import numpy as np\n",
    "# import icepyx as ipx\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import h5py\n",
    "import matplotlib.pylab as plt\n",
    "from math import *\n",
    "import random\n",
    "# import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import datetime as dt\n",
    "from shapely.geometry import Point\n",
    "import geopandas\n",
    "import scipy.stats as stats\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset    # Note: python is case-sensitive!\n",
    "from netCDF4 import date2num,num2date\n",
    "\n",
    "from pyproj import Proj, transform\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from functions import *\n",
    "from readers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb586019-c6d6-4eaa-be46-5b0dfde868a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', <class 'netCDF4._netCDF4.Dimension'>: name = 'x', size = 100)\n",
      "('y', <class 'netCDF4._netCDF4.Dimension'>: name = 'y', size = 100)\n",
      "('time', <class 'netCDF4._netCDF4.Dimension'> (unlimited): name = 'time', size = 0)\n",
      "entire S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1169/1169 [1:48:56<00:00,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', <class 'netCDF4._netCDF4.Dimension'>: name = 'x', size = 100)\n",
      "('y', <class 'netCDF4._netCDF4.Dimension'>: name = 'y', size = 100)\n",
      "('time', <class 'netCDF4._netCDF4.Dimension'> (unlimited): name = 'time', size = 0)\n",
      "entire S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1368/1368 [1:58:51<00:00,  5.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', <class 'netCDF4._netCDF4.Dimension'>: name = 'x', size = 100)\n",
      "('y', <class 'netCDF4._netCDF4.Dimension'>: name = 'y', size = 100)\n",
      "('time', <class 'netCDF4._netCDF4.Dimension'> (unlimited): name = 'time', size = 0)\n",
      "entire S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1337/1337 [1:53:58<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', <class 'netCDF4._netCDF4.Dimension'>: name = 'x', size = 100)\n",
      "('y', <class 'netCDF4._netCDF4.Dimension'>: name = 'y', size = 100)\n",
      "('time', <class 'netCDF4._netCDF4.Dimension'> (unlimited): name = 'time', size = 0)\n",
      "entire S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1376/1376 [1:55:02<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', <class 'netCDF4._netCDF4.Dimension'>: name = 'x', size = 100)\n",
      "('y', <class 'netCDF4._netCDF4.Dimension'>: name = 'y', size = 100)\n",
      "('time', <class 'netCDF4._netCDF4.Dimension'> (unlimited): name = 'time', size = 0)\n",
      "entire S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1227/1227 [1:18:46<00:00,  3.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# All ATL10 files stored in a folder\n",
    "\n",
    "th_fb = 0.8\n",
    "ib_filtering = True\n",
    "\n",
    "for target_year in [2019, 2020, 2021, 2022, 2023]:\n",
    "# for th_fb in [0.8, 1.0]:\n",
    "\n",
    "    files = glob.glob(f\"D:\\\\Floes\\\\ATL10\\\\ATL10-02_{target_year}*.h5\")\n",
    "\n",
    "    # Test for three regions of the Weddell Sea\n",
    "    for region in [\"entire\"]:\n",
    "        \n",
    "        # Test different lead types\n",
    "        lead_types = [\"S\"]\n",
    "        bbox = [-62, -77.6, -15, -60]\n",
    "    \n",
    "        for lead_type in lead_types: # \"SD\", \"S\", \"F001\", \"F002\"\n",
    "    \n",
    "            nc_name = 'D:\\\\IS2_topo_DL\\\\Ridges_density_Weddell_{0}.nc'.format(target_year)\n",
    "            \n",
    "            with Dataset(nc_name, mode='w') as ncfile:\n",
    "                \n",
    "                # INITIALIZE NC FILE ==================================================================\n",
    "                # Make grid for Weddell Sea (projection: NSIDC sea ice polar stereographic South - EPSG 3412)\n",
    "                wide = 2500000\n",
    "                y0 = 500000\n",
    "                x0 = -2800000\n",
    "                xp = np.arange(x0, x0+wide, 25000)\n",
    "                yp = np.arange(y0, y0+wide, 25000)\n",
    "    \n",
    "                xx, yy = np.meshgrid(xp, yp)\n",
    "                lats, lons = transform(Proj(\"EPSG:3412\"), Proj(\"EPSG:4326\"), xx, yy) \n",
    "    \n",
    "                x_len = len(xp)\n",
    "                y_len = len(yp)\n",
    "                \n",
    "                # CREATE NC FILE VARIABLES ================================\n",
    "                x_dim = ncfile.createDimension('x', x_len)     # latitude axis\n",
    "                y_dim = ncfile.createDimension('y', y_len)    # longitude axis\n",
    "                time_dim = ncfile.createDimension('time', None) # unlimited axis (can be appended to).\n",
    "                for dim in ncfile.dimensions.items():\n",
    "                    print(dim)\n",
    "    \n",
    "                ## XY grid ================================================\n",
    "                x = ncfile.createVariable('x', np.float32, ('x',))\n",
    "                x.units = 'meters'\n",
    "                x.long_name = 'Center_x_values'\n",
    "                y = ncfile.createVariable('y', np.float32, ('y',))\n",
    "                y.units = 'meters'\n",
    "                y.long_name = 'Center_y_values'\n",
    "    \n",
    "                ## Time ================================================\n",
    "                time = ncfile.createVariable('time', np.float64, ('time',))\n",
    "                time.units = 'hours since 1800-01-01'\n",
    "                time.long_name = 'time'\n",
    "    \n",
    "                ## latitude & longitude ================================================\n",
    "                latitude = ncfile.createVariable('lat', np.float64,('y','x',)) # note: unlimited dimension is leftmost\n",
    "                latitude.units = 'degrees_north' # degrees Kelvin\n",
    "                latitude.long_name = \"latitude\"\n",
    "    \n",
    "                longitude = ncfile.createVariable('lon', np.float64,('y','x',)) # note: unlimited dimension is leftmost\n",
    "                longitude.units = 'degrees_east' # degrees Kelvin\n",
    "                longitude.long_name = \"longitude\"\n",
    "    \n",
    "                ## Data variables ================================================\n",
    "                fb_mode = ncfile.createVariable('fb_mode', np.float64,('time','x','y')) # note: unlimited dimension is leftmost\n",
    "                fb_mode.units = 'meters' # degrees meter\n",
    "                fb_mode.standard_name = 'modal_freeboard' # this is a CF standard name\n",
    "                \n",
    "                fb_mean = ncfile.createVariable('fb_mean', np.float64,('time','x','y')) # note: unlimited dimension is leftmost\n",
    "                fb_mean.units = 'meters' # degrees meter\n",
    "                fb_mean.standard_name = 'mean_freeboard' # this is a CF standard name\n",
    "                \n",
    "                fb_med = ncfile.createVariable('fb_med', np.float64,('time','x','y')) # note: unlimited dimension is leftmost\n",
    "                fb_med.units = 'meters' # degrees meter\n",
    "                fb_med.standard_name = 'median_freeboard' # this is a CF standard name\n",
    "                \n",
    "                fb_std = ncfile.createVariable('fb_std', np.float64,('time','x','y')) # note: unlimited dimension is leftmost\n",
    "                fb_std.units = 'meters' # degrees meter\n",
    "                fb_std.standard_name = 'standard_deviation_freeboard' # this is a CF standard name\n",
    "    \n",
    "                fr_ridge = ncfile.createVariable('fr_ridge', np.float64,('time','x','y')) # note: unlimited dimension is leftmost\n",
    "                # fr_ridge.units = 'meters' # no units\n",
    "                fr_ridge.standard_name = 'ridge_fraction' # this is a CF standard name\n",
    "    \n",
    "                h_ridge = ncfile.createVariable('h_ridge', np.float64,('time','x','y')) # note: unlimited dimension is leftmost\n",
    "                h_ridge.units = 'meters' # degrees meter\n",
    "                h_ridge.standard_name = 'mean_ridge_height' # this is a CF standard name\n",
    "    \n",
    "                fr_lead = ncfile.createVariable('fr_lead', np.float64,('time','x','y')) # note: unlimited dimension is leftmost\n",
    "                # fr_lead.units = 'meters' # no units\n",
    "                fr_lead.standard_name = 'lead_fraction' # this is a CF standard name\n",
    "\n",
    "                fb_count = ncfile.createVariable('fb_count', np.float64,('time','x','y')) # note: unlimited dimension is leftmost\n",
    "                # fr_lead.units = 'meters' # no units\n",
    "                fb_count.standard_name = 'Number of ATL10 freeboard measurement points' # this is a CF standard name\n",
    "    \n",
    "                ib_cnt = ncfile.createVariable('ib_cnt', np.float64,('time','x','y')) # note: unlimited dimension is leftmost\n",
    "                # floe_cnt.units = 'meters' # no units\n",
    "                ib_cnt.standard_name = 'number of icebergs/landfast ice' # this is a CF standard name\n",
    "                \n",
    "                x[:] = xp\n",
    "                y[:] = yp\n",
    "    \n",
    "                latitude[:] = lats\n",
    "                longitude[:] = lons\n",
    "                ## ============================================================================\n",
    "                \n",
    "                nc_idx = 0\n",
    "                \n",
    "                print(region, lead_type)\n",
    "    \n",
    "                # Initializing freeboard distribution bin ranges\n",
    "                step = 0.1\n",
    "                ITD_bin_ranges = np.arange(0, 4.0+step, step) #np.arange(0.1, 1.5, num=50)\n",
    "                ITD_bins = 0.5*(ITD_bin_ranges[0:-1] + ITD_bin_ranges[1:])\n",
    "    \n",
    "                #### ITD parameters ####\n",
    "                cnt_file = 0\n",
    "                ITD_all = np.zeros((len(ITD_bins),len(files)))\n",
    "                year_all = np.zeros(len(files))\n",
    "                month_all = np.zeros(len(files))\n",
    "                day_all = np.zeros(len(files))\n",
    "                valid_idx = []\n",
    "    \n",
    "                #### Floe parameters ####\n",
    "                floe_year = np.array([])\n",
    "                floe_month = np.array([])\n",
    "                floe_day = np.array([])\n",
    "                floe_len = np.array([])\n",
    "                floe_lat = np.array([])\n",
    "                floe_lon = np.array([])\n",
    "                floe_fb_mean = np.array([])\n",
    "                floe_fb_med = np.array([])\n",
    "                floe_fb_std = np.array([])\n",
    "                lead_width = np.array([])\n",
    "                lead_position = np.array([])\n",
    "                nprof = 50\n",
    "    \n",
    "                #### Lead parameters ####\n",
    "                lead_width_bin_ranges = np.arange(10, 3000, 10)\n",
    "                lead_width_bin_means = 0.5*(lead_width_bin_ranges[0:-1] + lead_width_bin_ranges[1:])\n",
    "                binned_lead_count_all = np.zeros((len(lead_width_bin_means), len(files)))\n",
    "                binned_lead_spacings_all = np.zeros((len(lead_width_bin_means), len(files)))\n",
    "    \n",
    "                first = True\n",
    "                \n",
    "                ## LOOP FOR ALL ATL10 FILES =================================================\n",
    "                for k in tqdm(range(0, len(files))):\n",
    "                    filename = files[k]\n",
    "                    date = os.path.basename(filename)[9:9+14]\n",
    "                    year = int(date[:4])\n",
    "                    month = int(date[4:6])\n",
    "                    day = int(date[6:8])\n",
    "                    valid_beam = 0\n",
    "    \n",
    "                    year_all[k] = year\n",
    "                    month_all[k] = month\n",
    "                    day_all[k] = day\n",
    "    \n",
    "                    first_beam = True\n",
    "                    first_floe = True\n",
    "                    first_ib = True\n",
    "                    \n",
    "                    # Initialize grid (for nc file) -----------------------------------------\n",
    "                    grid_fb_mode = np.zeros(np.shape(xx)) * np.nan\n",
    "                    grid_fb_std = np.zeros(np.shape(xx)) * np.nan\n",
    "                    grid_fr_ridge = np.zeros(np.shape(xx)) * np.nan\n",
    "                    grid_h_ridge = np.zeros(np.shape(xx)) * np.nan\n",
    "                    grid_fr_lead = np.zeros(np.shape(xx)) * np.nan\n",
    "                    grid_floe_leng = np.zeros(np.shape(xx)) * np.nan\n",
    "                    grid_floe_cnt = np.zeros(np.shape(xx)) * np.nan\n",
    "                    grid_ib_cnt = np.zeros(np.shape(xx)) * np.nan\n",
    "                \n",
    "                    polygons = []\n",
    "                    w = 12500\n",
    "                    for xi in xp[:]:\n",
    "                        for yi in yp[:]:\n",
    "                            polygons.append(Polygon([(xi-w,yi-w), (xi+w, yi-w), (xi+w, yi+w), (xi-w, yi+w)]))\n",
    "    \n",
    "                    grid = geopandas.GeoDataFrame({'geometry':polygons}, crs = \"EPSG:3412\")\n",
    "                    grid['id'] = grid.index\n",
    "                    # -----------------------------------------------------------------\n",
    "                    \n",
    "                    df2 = pd.DataFrame({'lat': [], 'lon': []})\n",
    "                    df_floe = pd.DataFrame({'lat': [], 'lon': [], 'floe_len': []})\n",
    "    \n",
    "                    df_raw = read_ATL10(filename, bbox)\n",
    "    \n",
    "                    if len(df_raw) > 0:\n",
    "    \n",
    "                        strong_beams = pd.unique(df_raw['beam'])\n",
    "        \n",
    "                        for beam in strong_beams:\n",
    "        \n",
    "                            df = df_raw.loc[df_raw['beam']==beam, :].reset_index(drop = True)\n",
    "                            \n",
    "                            ib_mask2, df_ib = determine_iceberg(df, th_fb = th_fb)\n",
    "                            df_ib2, ib_mask3 = combine_icebergs(df, df_ib, ib_mask2, th_fb)\n",
    "    \n",
    "                            ########## APPLY Iceberg Filting (YES or NO) ##########\n",
    "                            if len(df_ib2) > 0:\n",
    "                                if first_ib:\n",
    "                                    df_ib_all = df_ib2\n",
    "                                    first_ib = False\n",
    "                                else:\n",
    "                                    df_ib_all = pd.concat([df_ib_all, df_ib2]).reset_index(drop = True)\n",
    "                                \n",
    "                            if ib_filtering:\n",
    "                                df = df[ib_mask3 == 0].reset_index(drop = True)\n",
    "                            ######################################################\n",
    "                            lat = df['lat'].values\n",
    "                            lon = df['lon'].values\n",
    "                            seg_x = df['seg_x'].values\n",
    "                            seg_len = df['seg_len'].values\n",
    "                            fb = df['fb'].values\n",
    "                            stype = df['stype'].values\n",
    "                            refsur_ndx = df['refsur_ndx'].values\n",
    "    \n",
    "                            freeboard_mode, sample_ndx = modal_profile(fb, seg_x, refsur_ndx)\n",
    "    \n",
    "                            ridge = np.zeros(np.shape(fb))\n",
    "                            # Ridge or not? (threshold 0.6 m above level (mode) freeboard)\n",
    "                            ridge[fb > freeboard_mode + 0.6] = 1\n",
    "        \n",
    "                            df['fb_mode'] = freeboard_mode\n",
    "                            df['ridge'] = ridge\n",
    "        \n",
    "                            if len(df) > 0 and np.sum(np.isnan(fb)) != len(fb):\n",
    "                                \n",
    "                                first_beam = False\n",
    "        \n",
    "                                if k not in valid_idx:\n",
    "                                    valid_idx.append(k)\n",
    "        \n",
    "                                ITD, _ = np.histogram(fb, bins=ITD_bin_ranges)\n",
    "                                ITD_all[:, k] += ITD\n",
    "                                \n",
    "                                # DETERMINE LEAD --------------------------------------------------\n",
    "                                lead_mask = np.zeros(len(fb))\n",
    "                                if lead_type == \"S\":  # Specular leads\n",
    "                                    lead_mask[(stype <= 5) & (stype>= 2) & (fb < 0.1)] = 1 \n",
    "                                elif lead_type == \"SD\":  # Specular + dark leads\n",
    "                                    lead_mask[(stype <= 9) & (stype>= 2) & (fb < 0.1)] = 1 \n",
    "                                elif lead_type == \"F001\":  # Freeboard heights (0.01 m)\n",
    "                                    lead_mask[fb < 0.01] = 1\n",
    "                                elif lead_type == \"F002\":  # Freeboard heights (0.02 m)\n",
    "                                    lead_mask[fb < 0.02] = 1\n",
    "                                # ------------------------------------------------------------------\n",
    "                                \n",
    "                                df['lead'] = lead_mask\n",
    "                                \n",
    "                                df = df.dropna().reset_index(drop = True)\n",
    "                                \n",
    "                                df2 = pd.concat([df2, df], ignore_index=True)\n",
    "                                \n",
    "                    if first_beam == False:\n",
    "                        \n",
    "                        # Combine the results to the NC file grid -------------------------------------\n",
    "                        gdf = geopandas.GeoDataFrame(df2, crs=\"EPSG:4326\",\n",
    "                                                     geometry=geopandas.points_from_xy(df2.lon, df2.lat)).to_crs(\"EPSG:3412\")\n",
    "    \n",
    "                        # gdf_floe = geopandas.GeoDataFrame(df_floe, crs=\"EPSG:4326\",\n",
    "                        #                                   geometry=geopandas.points_from_xy(df_floe.lon, df_floe.lat)).to_crs(\"EPSG:3412\")\n",
    "                        # gdf_floe = gdf_floe.sjoin(grid, how=\"left\").dropna()\n",
    "    \n",
    "                        left_df = gdf.sjoin(grid, how=\"left\").dropna()\n",
    "    \n",
    "                        ridge = left_df.groupby(\"id\").apply(calculate_ridge)\n",
    "                        array_fb_mode = [s[0] for s in ridge.values]\n",
    "                        array_fr_ridge = [s[1] for s in ridge.values]\n",
    "                        array_h_ridge = [s[2] for s in ridge.values]\n",
    "                        array_fb_mean = [s[3] for s in ridge.values]\n",
    "                        array_fb_med = [s[4] for s in ridge.values]\n",
    "                        array_fb_std = [s[5] for s in ridge.values]\n",
    "                        array_fr_lead = left_df.groupby(\"id\")[\"lead\"].apply(calculate_lead).values\n",
    "                        array_fb_count = left_df.groupby(\"id\")[\"lat\"].count()\n",
    "                        \n",
    "                        # array_floe_len = gdf_floe.groupby(\"id\")[\"floe_len\"].mean()\n",
    "                        # array_floe_cnt = gdf_floe.groupby(\"id\")[\"lat\"].count()\n",
    "    \n",
    "                        grid.loc[ridge.index, \"fb_mode\"] = array_fb_mode\n",
    "                        grid.loc[ridge.index, \"fr_ridge\"] = array_fr_ridge\n",
    "                        grid.loc[ridge.index, \"h_ridge\"] = array_h_ridge\n",
    "                        grid.loc[ridge.index, \"fb_mean\"] = array_fb_mean\n",
    "                        grid.loc[ridge.index, \"fb_med\"] = array_fb_med\n",
    "                        grid.loc[ridge.index, \"fb_std\"] = array_fb_std\n",
    "                        grid.loc[ridge.index, \"fr_lead\"] = array_fr_lead\n",
    "                        grid.loc[ridge.index, \"fb_count\"] = array_fb_count\n",
    "                        # grid.loc[array_floe_cnt.index, \"floe_len\"] = array_floe_len.values\n",
    "                        # grid.loc[array_floe_cnt.index, \"floe_cnt\"] = array_floe_cnt.values\n",
    "    \n",
    "                        fb_mode[nc_idx ,:,:] = grid[\"fb_mode\"].values.reshape(np.shape(xx)) #.transpose()\n",
    "                        fb_mean[nc_idx ,:,:] = grid[\"fb_mean\"].values.reshape(np.shape(xx)) #.transpose()\n",
    "                        fb_med[nc_idx ,:,:] = grid[\"fb_med\"].values.reshape(np.shape(xx)) #.transpose()\n",
    "                        fb_std[nc_idx ,:,:] = grid[\"fb_std\"].values.reshape(np.shape(xx)) #.transpose()\n",
    "                        fr_ridge[nc_idx ,:,:] = grid[\"fr_ridge\"].values.reshape(np.shape(xx)) #.transpose()\n",
    "                        h_ridge[nc_idx ,:,:] = grid[\"h_ridge\"].values.reshape(np.shape(xx)) #.transpose()\n",
    "                        fr_lead[nc_idx ,:,:] = grid[\"fr_lead\"].values.reshape(np.shape(xx)) #.transpose()\n",
    "                        fb_count[nc_idx,:,:] = grid[\"fb_count\"].values.reshape(np.shape(xx))\n",
    "                        \n",
    "                        # floe_leng[nc_idx ,:,:] = grid[\"floe_len\"].values.reshape(np.shape(xx)).transpose()\n",
    "                        # floe_cnt[nc_idx ,:,:] = grid[\"floe_cnt\"].values.reshape(np.shape(xx)).transpose()\n",
    "                        time[nc_idx] = date2num(dt.datetime.strptime(date, \"%Y%m%d%H%M%S\"), time.units)\n",
    "                        \n",
    "                        if first_ib == False:\n",
    "        \n",
    "                            gdf_ib = geopandas.GeoDataFrame(df_ib_all, crs=\"EPSG:4326\",\n",
    "                                                            geometry=geopandas.points_from_xy(df_ib_all.lon, df_ib_all.lat)).to_crs(\"EPSG:3412\")\n",
    "                            gdf_ib = gdf_ib.sjoin(grid, how=\"left\").dropna()\n",
    "        \n",
    "                            array_ib_cnt = gdf_ib.groupby(\"id\")[\"lat\"].count()    \n",
    "                            grid.loc[array_ib_cnt.index, \"ib_cnt\"] = array_ib_cnt.values\n",
    "                            grid_ib_cnt = grid[\"ib_cnt\"].values.reshape(np.shape(xx)).transpose()\n",
    "                            \n",
    "                        ib_cnt[nc_idx ,:,:] = grid_ib_cnt\n",
    "    \n",
    "                        nc_idx += 1\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4db680-98e5-4cd6-8048-52852d0c7e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
